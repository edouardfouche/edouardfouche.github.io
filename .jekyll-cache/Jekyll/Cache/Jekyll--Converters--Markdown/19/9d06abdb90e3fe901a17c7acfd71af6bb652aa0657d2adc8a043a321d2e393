I"[å<p>Outlier detection has the goal to reveal unusual patterns in data. Typical scenarios in predictive maintenance are the identification of failures, sensor malfunctions or intrusions. This is a challenging task, especially when the data is high-dimensional, because outliers become <em>hidden</em> and are visible only in particular subspaces. In this article, I will discuss approaches based on auto-encoder to discover outliers in high-dimensional data.</p>

<p><img src="/img/neural-based-outlier-discovery/autoencoder.png" alt="autoencoder" class="img-responsive" /></p>

<h2 id="introduction">Introduction</h2>

<p>Let‚Äôs start with an example. By looking at the plot hereafter, it is easy to see that the red points are outlying in the space spanned by the variables x, y and z.</p>

<p><img src="/img/neural-based-outlier-discovery/3d-plot-outlier.svg" alt="3d-plot-outlier" class="img-responsive" /></p>

<p>However, if we project the data in lower dimensional subspaces, their outlying behavior is lost. We say that such outliers are <em>non-trivial</em> <sup id="fnref:fn2" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">1</a></sup>.</p>

<p><img src="/img/neural-based-outlier-discovery/2d-plot-outlier.svg" alt="2d-plot-outlier" class="img-responsive" /></p>

<p>Also, the same points might not be seen as outliers in other dimensions, such as [r,s,t].</p>

<p><img src="/img/neural-based-outlier-discovery/3d-plot-non-outlier.svg" alt="3d-plot-non-outlier" class="img-responsive" /></p>

<p>When the data is high-dimensional, i.e. each data point is described by a large vector (let‚Äôs say 100, 1000 or more values), they cannot be detected in the full data space either. This is an artifact of the so-called <em>curse of dimensionality</em>: the space becomes sparsely populated and the notions of neighborhood becomes meaningless<sup id="fnref:fn3" role="doc-noteref"><a href="#fn:fn3" class="footnote" rel="footnote">2</a></sup>, such that full space outlier detection shows poor results. The outlying behavior of data points can be observed in subspaces only. This is problematic, because the number of subspaces grows exponentially with the number of dimensions of a data set: We cannot afford to explore all of them.</p>

<h2 id="auto-encoder">Auto-encoder</h2>

<p>One can use an auto-encoder to detect outliers. An auto-encoder, also formerly called <em>replicator network</em>, is an unsupervised neural networks. Basically, it consists in learning a pair of non-linear transformations: A mapping from the original space to another space (encoder) - of possibly higher or lower dimensionality - and a mapping back from this new space to the original one (decoder). The assumption is, since outliers are <em>rare</em> and <em>different</em>, that the auto-encoder will not learn to map those objects correctly, inducing a higher reconstruction error. By looking at the reconstruction errors, one can infer an outlier score.</p>

<p>Interestingly, there is no need for labels, i.e. we don‚Äôt need to get example of <em>normal</em> and <em>abnormal</em> points. The network can infer directly from the data points a degree of abnormality with respect to the other points. This is what we may call a <em>pure</em> unsupervised setting. This is convenient in anomaly detection, because the characteristics of anomalies/outliers are in general not known in advance. It turns out that this method works quite well, as shown in previous work<sup id="fnref:fn4" role="doc-noteref"><a href="#fn:fn4" class="footnote" rel="footnote">3</a></sup> <sup id="fnref:fn5" role="doc-noteref"><a href="#fn:fn5" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:fn6" role="doc-noteref"><a href="#fn:fn6" class="footnote" rel="footnote">5</a></sup>.</p>

<p>Nonetheless, high-dimensionality represents a major obstacle to outlier detection. Since auto-encoder learn a mapping to a possibly lower space, there are good reasons to think they would scale to high-dimensional problems. Unfortunately, such aspect was overlooked in the existing literature. Also, the typical benchmark data sets are low dimensional (with a maximum of 30 to 40 dimensions).</p>

<p>So, let‚Äôs try out. We are using one of the HiCS<sup id="fnref:fn2:1" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">1</a></sup> synthetic <a href="https://www.ipd.kit.edu/~muellere/HiCS/">data sets</a>. The data set contains 100 dimensions and 1000 instances, with 136 of them being outliers. The data was generated such that outliers are hidden, i.e. they are visible only in particular subspaces. In the example above, we actually looked at the subspace [30,31,32] of this data set, which contains 5 outliers.</p>

<p>Let‚Äôs open the data set as a Pandas.DataFrame:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.io</span> <span class="kn">import</span> <span class="n">arff</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> 

<span class="n">data</span><span class="p">,</span> <span class="n">meta</span> <span class="o">=</span> <span class="n">arff</span><span class="p">.</span><span class="n">loadarff</span><span class="p">(</span><span class="s">"synth_multidim_100_000.arff"</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>We preprocess and save the labels for later. Note that we delete the labels from the original data set, because training with them would be like cheating !</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'class'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">labels</span><span class="p">[</span><span class="n">labels</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">del</span> <span class="n">data</span><span class="p">[</span><span class="s">'class'</span><span class="p">]</span>
</code></pre></div></div>

<p>We normalize the data between 0 and 1:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">min_max_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">np_scaled</span> <span class="o">=</span> <span class="n">min_max_scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_n</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np_scaled</span><span class="p">)</span>
<span class="n">data_n</span> <span class="o">=</span> <span class="n">data_n</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we can start building a simple autoencoder. In this example, we will use <a href="https://keras.io/">Keras</a>. We build an autoencoder with a single hidden layer containing 80 neurons, and an input and output layer of size 100. We use <em>ReLU</em> as activation function in the hidden layer and <em>Sigmoid</em> in the output layer. We choose <em>adadelta</em> as gradient optimizer and <em>binary_crossentropy</em> as loss function.</p>

<p>With just a few lines of code, we can starting training this network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">encoding_dim</span> <span class="o">=</span> <span class="mi">80</span> 
<span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,))</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">decoded</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">encoded</span><span class="p">)</span>

<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,))</span>
<span class="n">decoder_layer</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">encoded_input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">decoder_layer</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">))</span>

<span class="n">autoencoder</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adadelta'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">)</span>

<span class="n">autoencoder</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_n</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">data_n</span><span class="p">.</span><span class="n">values</span><span class="p">,</span>
                <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>We train over 2500 epochs, with a batch size of 100. Finally, we get the prediction of the network for our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_n</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
</code></pre></div></div>

<p>We compute the euclidean distance from each point to its reconstruction. We use it as an outlier score:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data_n</span><span class="p">.</span><span class="n">values</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_n</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">decoded</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>

<p>Then, to assess the quality of our outlier detector, we plot the ROC curve:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'AUC = %0.2f)'</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'navy'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'ROC Autoencoder 100-80-100 ReLU/Sigmoid synth\_multidim\_100\_000'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-training-roc.svg" alt="ae-outlier-training-roc" class="img-responsive" /></p>

<p>An area under the curve (AUC) of 0.91 is actually very good. As a comparison, state-of-the-art methods, such as HiCS<sup id="fnref:fn2:2" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">1</a></sup>, only obtain an AUC of about 0.8 on this data set.</p>

<p>It is interesting to plot the outlier scores of each single data point:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
<span class="n">data</span><span class="p">[</span><span class="s">'dist'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dist</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s">'dist'</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'labels'</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Index'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Score'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Outlier Score"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-training.svg" alt="ae-outlier-training" class="img-responsive" /></p>

<p>Dark points are the outliers, inferred from the labels. As we can see, they get in general a higher score than non-outliers. The effect is even more visible if we compare the distance before and after learning. Before learning, the network weights are initialized randomly. The network has not learned any transformation yet so it performs very poorly at the reconstruction of each data points, independent of them being outliers or not.</p>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-training-comp.svg" alt="ae-outlier-training-comp" class="img-responsive" /></p>

<h2 id="interpretability">Interpretability</h2>

<p>The outlier score is a useful information about the <em>abnormality</em> of data points. However, in practice this is often not enough. One does not only want to know which points are outliers, but also why these points are so special, i.e. in which subspaces they are abnormal.</p>

<p>The lack of interpretation possibilities has been a major criticism of neural network. Neural networks are often seen as a magic black box doing great things but excluding any interpretation possibilities. I would say that in the case of outlier detection, this is not true.</p>

<p>By looking at the reconstruction error, we can find hints about the dimensions in which a particular data point is outlying. For example, point 350 is one of the outlier in the subspace [30,31,32], we can compute its reconstruction error per dimension and plot it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_error_per_dim</span><span class="p">(</span><span class="n">point</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">data_n</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">point</span><span class="p">,:]).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">decoded</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">350</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Index'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Reconstruction error'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Reconstruction error in each dimension of point 350"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-reconstruction-350.svg" alt="ae-outlier-reconstruction-350" class="img-responsive" /></p>

<p>As we can see, the point 350 shows a higher reconstruction error in dimensions [30,31,32], suggesting correctly that it is outlying in this subspace.</p>

<p>Similarly, point 50, which is an outlier in subspace [30,31,32] and [68,69], shows a high reconstruction error in dimensions [30,31,32,68,69].</p>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-reconstruction-50.svg" alt="ae-outlier-reconstruction-50" class="img-responsive" /></p>

<p>Point 50 also has a relative high reconstruction error in subspace [50,51,52]. This is a false positive, because point 50 does not show a clear outlier behavior in this subspace. Still, it seems that point 50 is located in a relatively sparse region in the subspace [50,51,52] (see the red point).</p>

<p><img src="/img/neural-based-outlier-discovery/3d-plot-non-outlier-50.svg" alt="3d-plot-non-outlier-50" class="img-responsive" /></p>

<p>So it seems that one can find back the information in which each object is an outlier by looking at the reconstruction errors, or at least reduce drastically the search space. Hereafter, we plot the reconstruction error of each outlier in the subspace [30,31,32]. Namely, the points 50, 121, 350, 572 and 559.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 50, 121, 350, 572 and 559 are outliers in subspace [30,31,32]
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"50"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"121"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">350</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"350"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">572</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"572"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">compute_error_per_dim</span><span class="p">(</span><span class="mi">669</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"669"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Index'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Reconstruction error'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Reconstruction error in each dimension of outliers in [30,31,32]"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/img/neural-based-outlier-discovery/ae-outlier-reconstruction-all.svg" alt="ae-outlier-reconstruction-all" class="img-responsive" /></p>

<p>The peak at [30,31,32] seems quite consistent for each point.</p>

<h2 id="what-about-deep-learning">What about Deep Learning?</h2>

<p>Nowadays, everybody is crazy about deep learning. Increasing the size of a neural network is often thought as a silver bullet to increase accuracy. Despite being true in many settings, and especially in computer vision tasks, this is not true here.</p>

<p>For example, having to many parameters would allow the network to overfit the data, such that all points would be reconstructed nearly perfectly, blurring out the differences between <em>normal</em> and <em>abnormal</em> data points. Previous work<sup id="fnref:fn5:1" role="doc-noteref"><a href="#fn:fn5" class="footnote" rel="footnote">4</a></sup> showed that one single layer brings better results in outlier detection. Controversely, <sup id="fnref:fn6:1" role="doc-noteref"><a href="#fn:fn6" class="footnote" rel="footnote">5</a></sup> argue that adding layers to <em>compress</em> gradually representation of the data helps. Obviously, there is a need for experiments here.</p>

<p>By chance, deep learning research leads the development of many gradient optimization and regularization methods that can be very useful for this task too.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article, we introduced the problems related to finding outlier in high-dimensional spaces. We show that neural-based approaches, such as auto-encoders, can provide great results.</p>

<p>Nonetheless, the quality of these results is conditioned by the choice of the parameters, such as the <em>number of hidden neurons</em>, <em>activation function</em>, <em>gradient optimization</em>, <em>number of training epochs</em>, <em>size of mini-batches</em>, <em>distance function</em>‚Ä¶ Most of the current work is based on trial-and-error, which is time consuming and may lead to poor generalization. It would be interesting to derive rules to fix these parameters based on the data, such as its correlation structure or the expected proportion/characteristics of outliers.</p>

<p>The results showed in this article are based on a single data set, with carefully chosen parameters. It would be difficult to infer the quality of the same model on different data sets with different characteristics. Other neural-based methods, such as the <em>Self-Organizing Maps</em><sup id="fnref:fn9" role="doc-noteref"><a href="#fn:fn9" class="footnote" rel="footnote">6</a></sup> or the <em>Restricted Boltzmann Machines</em><sup id="fnref:fn10" role="doc-noteref"><a href="#fn:fn10" class="footnote" rel="footnote">7</a></sup> also shows promising results and would be interesting to compare.</p>

<h2 id="try-it-yourself-">Try it yourself !</h2>

<p>The underlying notebook is available on my github, <a href="https://github.com/edouardfouche/neural-based-outlier-discovery/blob/master/autoencoder-keras.ipynb">here</a>. You are welcome to try out and play around with the parameters. Don‚Äôt hesitate to share your results if you find something interesting !</p>

<h2 id="sources">Sources</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn2" role="doc-endnote">
      <p>Keller F., M√ºller E. &amp; B√∂hm K. (2012). <em>HiCS: High contrast subspaces for density-based outlier ranking</em>. International Conference on Data Engineering.¬†<a href="#fnref:fn2" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:fn2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:fn2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:fn3" role="doc-endnote">
      <p>Beyer K., Goldstein J., Ramakrishnan R. &amp; Shaft U. (1999). <em>When is ‚Äúnearest neighbor‚Äù meaningful?</em>. International Conference on Database Theory.¬†<a href="#fnref:fn3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn4" role="doc-endnote">
      <p>Hawkins S., He H., Williams G. &amp; Baxter R. (2002). <em>Outlier Detection Using Replicator Neural Networks</em>. Data Warehousing and Knowledge Discovery.¬†<a href="#fnref:fn4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn5" role="doc-endnote">
      <p>Dau H. A., Ciesielski V. &amp; Song A. (2014). <em>Anomaly Detection Using Replicator Neural Networks Trained on Examples of One Class</em>. Simulated Evolution and Learning.¬†<a href="#fnref:fn5" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:fn5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn6" role="doc-endnote">
      <p>An J., Cho S. (2016). <em>Variational Autoencoder based Anomaly Detection using Reconstruction Probability</em>. CoRR.¬†<a href="#fnref:fn6" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:fn6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn9" role="doc-endnote">
      <p>Mu√±oz A. &amp; Muruz√°bal J. (1998). <em>Self-organizing maps for outlier detection</em>. Neurocomputing.¬†<a href="#fnref:fn9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn10" role="doc-endnote">
      <p>Fiore U., Palmieri F., Castiglione A. &amp; De Santis A. (2013). <em>Network anomaly detection with the restricted Boltzmann machine</em>. Neurocomputing.¬†<a href="#fnref:fn10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET