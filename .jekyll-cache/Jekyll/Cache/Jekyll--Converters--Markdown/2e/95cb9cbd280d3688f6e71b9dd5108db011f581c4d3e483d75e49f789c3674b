I"tc<p>I came across the <em>im2txt</em> model for <em>Tensorflow</em>. It is a model developed by <em>Google DeepMind</em> that takes an image as input and creates a caption for it. I used the model to create captions for a few of my own images, and it was a lot of fun ! In this article, I will explain how to play with it. Then, I will show a few examples.</p>

<h2 id="about-the-model">About the model</h2>

<p>This model won ex-aequo with a team from Microsoft Research an image captioning competition based on the <a href="http://mscoco.org/">COCO</a> data set. Generating caption for images is an interesting problem, since it leads to a better indexing of images on the web and to the possibility for computers to “understand” images.</p>

<p>Basically, a deep convolutional neural network is used to encode images to a fixed-length vector representation. It is followed by a long short-term memory (LSTM) network, which takes the encoding and produces a caption.</p>

<p>I am not going to describe it in detail here. More information can be found in the <a href="https://github.com/tensorflow/models/tree/master/im2txt">github</a> and the <a href="https://arxiv.org/abs/1609.06647">paper</a> of the model<sup id="fnref:fn3" role="doc-noteref"><a href="#fn:fn3" class="footnote" rel="footnote">1</a></sup>. Now let’s move to the fun part !</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>We need to clone the <code class="language-plaintext highlighter-rouge">tensorflow/models</code> folder which contains <code class="language-plaintext highlighter-rouge">im2txt</code> somewhere in our machine. Personally, I put all my git projects in a <code class="language-plaintext highlighter-rouge">~/git</code> folder:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>edouard@tower:~/git<span class="nv">$ </span>git clone https://github.com/tensorflow/models.git
</code></pre></div></div>
<p>In this small tutorial, I used the version at commit <a href="https://github.com/tensorflow/models/tree/c22611891d0826bbf656a367874489c0dad95777">c226118</a>. This is the current version at the writing time of this article, but future versions might work too.</p>

<p>The following packages are required (<a href="https://github.com/tensorflow/models/tree/master/im2txt#install-required-packages">instructions</a>):</p>
<ul>
  <li>Bazel</li>
  <li>Tensorflow (version 0.12.1)</li>
  <li>Numpy</li>
  <li>Natural Languages Toolkit (NLTK)</li>
</ul>

<p>Since we don’t want to train the model by ourself (it would take weeks), we download a fine-tuned <em>checkpoint</em>, as well as a the corresponding vocabulary file. The checkpoint I found was trained over 3 millions iterations. I recommend to put the <code class="language-plaintext highlighter-rouge">model.ckpt-3000000</code>, <code class="language-plaintext highlighter-rouge">model.ckpt-3000000.meta</code> and <code class="language-plaintext highlighter-rouge">word_counts.txt</code> files in a distinct folder, such as <code class="language-plaintext highlighter-rouge">~/im2txt/</code>.</p>

<ul>
  <li>Download the <em>checkpoint</em> from <a href="https://drive.google.com/file/d/0B_qCJ40uBfjEWVItOTdyNUFOMzg/view">here</a>.</li>
</ul>

<h2 id="run-the-inference">Run the inference</h2>

<p>First, we need to build the <code class="language-plaintext highlighter-rouge">run_inference</code> module with Bazel:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>edouard@tower:~<span class="nv">$ </span><span class="nb">cd</span> ~/git/models/im2txt
edouard@tower:~/git/models/im2txt<span class="nv">$ </span>bazel build <span class="nt">-c</span> opt im2txt/run_inference
..........
INFO: Found 1 target...
Target //im2txt:run_inference up-to-date:
  bazel-bin/im2txt/run_inference
INFO: Elapsed <span class="nb">time</span>: 3.078s, Critical Path: 0.02s
</code></pre></div></div>

<p>Hint: You should make sure that your <code class="language-plaintext highlighter-rouge">$JAVA_HOME</code> variable is set correctly. I have the following declaration in my <code class="language-plaintext highlighter-rouge">~/.bashrc</code> file:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span><span class="s2">"/usr/lib/jvm/java-8-openjdk-amd64"</span>
</code></pre></div></div>

<p>Then, you should be good to go ! I placed the image I wanted to caption (<code class="language-plaintext highlighter-rouge">img.jpg</code>) in the <code class="language-plaintext highlighter-rouge">~/im2txt/</code> folder and did the following:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>edouard@tower:~<span class="nv">$ CHECKPOINT_DIR</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOME</span><span class="k">}</span><span class="s2">/im2txt/model.ckpt-3000000"</span>
edouard@tower:~<span class="nv">$ IMAGE_FILE</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOME</span><span class="k">}</span><span class="s2">/im2txt/img.jpg"</span>
edouard@tower:~<span class="nv">$ VOCAB_FILE</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOME</span><span class="k">}</span><span class="s2">/im2txt/word_counts.txt"</span>

edouard@tower:~<span class="nv">$ </span><span class="nb">cd</span> ~/git/models/im2txt

edouard@tower:~/git/models/im2txt<span class="nv">$ </span>bazel-bin/im2txt/run_inference <span class="se">\</span>
  				   <span class="nt">--checkpoint_path</span><span class="o">=</span><span class="k">${</span><span class="nv">CHECKPOINT_DIR</span><span class="k">}</span> <span class="se">\</span>
                                   <span class="nt">--vocab_file</span><span class="o">=</span><span class="k">${</span><span class="nv">VOCAB_FILE</span><span class="k">}</span> <span class="se">\</span>
                                   <span class="nt">--input_files</span><span class="o">=</span><span class="k">${</span><span class="nv">IMAGE_FILE</span><span class="k">}</span>
</code></pre></div></div>

<p>This will probably not work on the first run. Depending on your Tensorflow/Python version, you might need to hack around a bit. For your information, I use Tensorflow 0.12.1 and Python 3.5.2. The modifications are mostly due to changes in the Tensorflow API and to Python2/3 differences.</p>

<p>I would recommend you to try the following changes only if Python complains:</p>

<ul>
  <li>In <code class="language-plaintext highlighter-rouge">~/git/im2txt/im2txt/show_and_tell_model.py</code>:</li>
</ul>

<p>At line 247:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span> <span class="c1">#to:
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lstm_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span>
</code></pre></div></div>

<p>At line 267:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">initial_state</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"initial_state"</span><span class="p">)</span> <span class="c1">#to:
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">initial_state</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"initial_state"</span><span class="p">)</span>
</code></pre></div></div>

<p>At line 273:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state_tuple</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">state_feed</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#to:
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state_tuple</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">state_feed</span><span class="p">,</span> <span class="n">num_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">split_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>At line 281:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">state_tuple</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"state"</span><span class="p">)</span> <span class="c1">#to:
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">state_tuple</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"state"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>In <code class="language-plaintext highlighter-rouge">~/git/im2txt/im2txt/inference_utils/vocabulary.py</code>:</li>
</ul>

<p>At line 49:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reverse_vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">reverse_vocab</span><span class="p">]</span> <span class="c1"># to:
</span></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reverse_vocab</span> <span class="o">=</span> <span class="p">[</span><span class="nb">eval</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]).</span><span class="n">decode</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">reverse_vocab</span><span class="p">]</span>
</code></pre></div></div>

<p>The following pages helped me finding the required modifications:</p>
<ul>
  <li><a href="https://github.com/tensorflow/tensorflow/issues/6432">rnn_cell has no BasicLSTMCell #6432</a></li>
  <li><a href="https://github.com/tensorflow/models/issues/466#issuecomment-254002590">Pretrained model for img2txt? #466</a></li>
</ul>

<p>When everything works, you get the top 3 results of what the network thinks your image represents, printed in the shell.</p>

<p>CPU Only:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:tensorflow:Building model.
INFO:tensorflow:Initializing vocabulary from file: /home/edouard/im2txt/word_counts.txt
INFO:tensorflow:Created vocabulary with 11520 words
INFO:tensorflow:Running caption generation on 1 files matching /home/edouard/im2txt/img.jpg
INFO:tensorflow:Loading model from checkpoint: /home/edouard/im2txt/model.ckpt-3000000
INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-3000000
Captions <span class="k">for </span>image img.jpg:
  0<span class="o">)</span> a man sitting on top of a wooden bench next to a tree <span class="nb">.</span> &lt;S&gt; &lt;S&gt; <span class="nb">.</span> &lt;S&gt; <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000005<span class="o">)</span>
  1<span class="o">)</span> a man sitting on top of a wooden bench next to a forest <span class="nb">.</span> &lt;S&gt; &lt;S&gt; <span class="nb">.</span> &lt;S&gt; <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000004<span class="o">)</span>
  2<span class="o">)</span> a man sitting on top of a wooden bench next to a tree <span class="nb">.</span> &lt;S&gt; &lt;S&gt; &lt;S&gt; <span class="nb">.</span> <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000004<span class="o">)</span>
edouard@tower:~/git/models/im2txt<span class="nv">$ </span>
</code></pre></div></div>

<p>With GPU:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node <span class="nb">read </span>from SysFS had negative value <span class="o">(</span><span class="nt">-1</span><span class="o">)</span>, but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 970
major: 5 minor: 2 memoryClockRate <span class="o">(</span>GHz<span class="o">)</span> 1.253
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.45GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device <span class="o">(</span>/gpu:0<span class="o">)</span> -&gt; <span class="o">(</span>device: 0, name: GeForce GTX 970, pci bus <span class="nb">id</span>: 0000:01:00.0<span class="o">)</span>
Captions <span class="k">for </span>image img.jpg:
  0<span class="o">)</span> a man sitting on top of a wooden bench next to a tree <span class="nb">.</span> &lt;S&gt; &lt;S&gt; <span class="nb">.</span> &lt;S&gt; <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000005<span class="o">)</span>
  1<span class="o">)</span> a man sitting on top of a wooden bench next to a forest <span class="nb">.</span> &lt;S&gt; &lt;S&gt; <span class="nb">.</span> &lt;S&gt; <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000004<span class="o">)</span>
  2<span class="o">)</span> a man sitting on top of a wooden bench next to a tree <span class="nb">.</span> &lt;S&gt; &lt;S&gt; &lt;S&gt; <span class="nb">.</span> <span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.000004<span class="o">)</span>
edouard@tower:~/git/models/im2txt<span class="nv">$ </span>
</code></pre></div></div>

<p>Creating  a caption for an image using the GPU takes about 8 seconds. Most of the time is spent loading the CUDA library, so it would be more efficient to create captions for a bunch of images in a row. This is possible, if you separate the paths to the files with a comma. With CPU only, it took about 12 seconds. As long as you don’t plan to train the model yourself, a modern CPU is just fine.</p>

<h2 id="results">Results</h2>

<p>I let the inference run on a bunch of pictures I took in the past. Sometimes, I’ve got very unexpected results and it was really funny ! When pictures are a bit abstract, or contain unusual objects, it fails. Here I would like to share a few of my good and no-so-good results.</p>

<p><br /></p>

<p class="center" style="max-width: 500px; margin: auto"><img src="/img/fun-with-tensorflow-im2txt/img.jpg" alt="img" class="img-responsive" /></p>
<p style="text-align: center"><strong>A man sitting on top of a wooden bench next to a tree.</strong></p>

<p>This one is very accurate. Just the formulation “on top of a wooden bench” sounds a bit robotic to me.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img2.jpg" alt="img2" class="img-responsive" /></p>

<p style="text-align: center"><strong>A group of people sitting around a table eating food</strong></p>

<p>This one is also great ! By the way, those two guys are my friends <a href="http://www.danielrojas.net/">Daniel</a> and <a href="http://www.maximlapis.com/">Maxim</a>. They are awesome !</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img18.jpg" alt="img18" class="img-responsive" /></p>

<p style="text-align: center"><strong>A bike is parked next to a fence.</strong></p>

<p>Very good too. Maybe it could have said something about the river and the trees.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img20.jpg" alt="img20" class="img-responsive" /></p>

<p style="text-align: center"><strong>A couple of birds that are standing in some water.</strong></p>

<p>Correct ! Though the water is in the background.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img27.jpg" alt="img27" class="img-responsive" /></p>

<p style="text-align: center"><strong>A close up of a television set with a keyboard.</strong></p>

<p>Actually, the object in the picture is a device used to analyze numerical circuits. The model certainly does not know what it is, but the approximation to a television makes a lot of sense. Also, the different keys together certainty look a bit like a keyboard.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img11.jpg" alt="img11" class="img-responsive" /></p>

<p style="text-align: center"><strong>A group of people standing on top of a sandy beach.</strong></p>

<p>This one is partly wrong. Obviously, it did not get the <a href="https://en.wikipedia.org/wiki/Tilt%E2%80%93shift_photography"><em>tilt-shift</em></a> effect that makes the scenery look miniaturized. However, because of the black &amp; white grain I understand why it thinks there is sand.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img9.jpg" alt="img9" class="img-responsive" /></p>

<p style="text-align: center"><strong>A close up of a small bird on a table.</strong></p>

<p>This is not so good. Clearly, it is an animal (actually, an <a href="https://en.wikipedia.org/wiki/Axolotl">axolotl</a>) but it is quite different from a bird. Also, I can’t see the table !</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img30.jpg" alt="img30" class="img-responsive" /></p>

<p style="text-align: center"><strong>A group of people standing next to a train in the background.</strong></p>

<p>There are indeed people in the background (monks praying, but they are really out of focus). I think it falsely interpreted the candles as people. The idea of a train makes some sense, because the iron candle rails remind a bit of train tracks.</p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img15.jpg" alt="img15" class="img-responsive" /></p>

<p style="text-align: center"><strong>A small bird sitting on top of a wooden bench.</strong></p>

<p>Oops ! It did not get the picture at all, probably because it is too abstract.</p>

<p><br /><br /></p>

<p><br /><br /></p>

<p><img src="/img/fun-with-tensorflow-im2txt/img31.jpg" alt="img31" class="img-responsive" /></p>

<p style="text-align: center"><strong>A close up of a person holding a pair of scissors.</strong></p>

<p>Maybe that’s the worst result I’ve got. That is probably because the model did not learn butterflies !</p>

<p><br /></p>

<h2 id="conclusion">Conclusion</h2>

<p>To conclude, I would say that the model performs quite well on a restricted range of examples. As long as the picture contains only common objects, it is able to produce meaningful captions. If the picture requires a certain level of abstraction, results are unpredictable.</p>

<p>This observation is not surprising, because the network was trained only with images containing simple objects at a relatively low level of abstraction: The model can only get at most as good as the data. At the following <a href="http://mscoco.org/explore/">link</a>, you can see the different objects of the COCO data set and query examples of images.</p>

<p>How could we do better? One solution would be to improve the training set. This might work, but would be very costly: Currently, the COCO data set contains several hundred thousands of images to represent 80 objects only. The number of images we would need to segment and annotate to represent the vast variety of the existing objects is intractable.</p>

<p>Also, it is unclear yet how to train neural networks to understand abstract concepts. Certainly, progress in other - but connected - areas are required. I believe that a fundamental shift should happen in the focus of the task: from <em>recognizing</em> to <em>interpreting</em>.</p>

<p>The key would be to make neural networks learn from fewer examples and to let them learn in an unsupervised way (we are speaking about the so-called <em>one-shot</em><sup id="fnref:fn1" role="doc-noteref"><a href="#fn:fn1" class="footnote" rel="footnote">2</a></sup> and <em>zero-shot</em><sup id="fnref:fn2" role="doc-noteref"><a href="#fn:fn2" class="footnote" rel="footnote">3</a></sup> learning problems). That is to say, they should learn without us telling them all over again the correct labels. For example, as humans, we don’t need to see several thousands of examples of cars to recognize cars.</p>

<p>Obviously, this is an open research problem.</p>

<p>I wanted to use the model to create captions for my own photographs. For example, I would like to have a bot that could go to my <a href="https://www.flickr.com/photos/edouardf">Flickr</a> or my <a href="https://www.facebook.com/edouardf">Facebook</a> and add automatically good captions to all the images I post. I must say it is rather unlikely for me to use the model to do that, because it may create a lot of meaningless tags. I would probably wait for better results !</p>

<h2 id="try-it-yourself-">Try it yourself !</h2>

<p>Using the instructions, I guess you should be able to run the model yourself with little work ! Don’t hesitate to share your results ! :)</p>

<h2 id="sources">Sources</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:fn3" role="doc-endnote">
      <p>Vinyals O., Toshev A., Bengio S. &amp; Erhan D. (2016). <em>Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence. <a href="#fnref:fn3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn1" role="doc-endnote">
      <p>Fei-Fei L., Fergus R., &amp; Perona P. (2006). <em>One-shot learning of object categories</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence. <a href="#fnref:fn1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:fn2" role="doc-endnote">
      <p>Palatucci M., Hinton G. E., Pomerleau D. &amp; Mitchell, T. M. (2009). <em>Zero-Shot Learning with Semantic Output Codes</em>. Neural Information Processing Systems. <a href="#fnref:fn2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET